
# coding: utf-8

# ## The Mklaren library: a Quick-start introduction
# 
# Mklaren is a library containing Multiple kernel learning and Kernel low-rank approximation methods, written in Python.
# 
# <br/>
# 
# <b>Features:</b>
# * Support for standard kernel functions (RBF, linear, polynomial, sigmoid)
# * Efficient interface to the kernel matrix
# * Low-rank kernel approximation methods (Incomplete Cholesky Decomposition, Cholesky with Side-information, the Nystrom method)
# * Multiple kernel learning methods based on centered alignment
# * Simultaneous multiple kernel learning and low-rank approximation base on least-angle regression (the Mklaren algorithm)
# 
# <br/>
# <br/>

# In[1]:


import os
#import pandas as pd
import numpy as np
os.chdir("C:/Users/da.salazarb/Google Drive/Tutorial_2018-10/03_Resultados/DataTCGA/")
from oct2py import Oct2Py
#octave = Oct2Py('C:\Octave\Octave-4.4.0\bin\octave-cli-4.4.0.exe')

#import mklaren
#import datasets

### OJO SE DEBE ALINEAR O CENTRAR!!!!!! Align()


#get_ipython().run_line_magic('matplotlib', 'inline')
#import matplotlib.pyplot as plt
from mklaren.mkl.mklaren import Mklaren
from mklaren.kernel.kernel import linear_kernel, poly_kernel, sigmoid_kernel, rbf_kernel, exponential_absolute, periodic_kernel, matern_kernel
from mklaren.regression.ridge import RidgeLowRank
from mklaren.projection.nystrom import Nystrom
from mklaren.projection.icd import ICD
from mklaren.kernel.kinterface import Kinterface
from sklearn.metrics import mean_squared_error
np.set_printoptions(precision=3)


# <br/>
# The input data is represented by a set pairs: 
# $$ \{(\mathbf{x}_1, y_1), (\mathbf{x}_2, y_2), ..., (\mathbf{x}_n, y_n)\}$$ 
# 
# where $\mathbf{x}_i$ represent independent variables (covariates) and $y_i$ are real-valued regression targets. The data is split in 50% (training set) and 50% (test set) in this example.

# <br/>
# ### The Kernel interface
# 
# The central data structure in the library is the ```Kinterface``` (Kernel interface) class. It mimics a Numpy array storing a kernel matrix generated by the ```kernel``` function (with parameters ```kernel_args```) on a finite ```data``` sample. The idea behind  ```Kinterface``` is the dynamic calculation of the kernel matrix values, as the usual kernel matrix approximation algorithms do not require access to the full kernel matrix.

# In[5]:

def ICDoneKernelOneTCGA(tcga, kernel, kernel_args, rank):
    
    #K = Kinterface(data=np.array(cnv), kernel=rfb_kernel, kernel_args={"sigma": 110})
    K = Kinterface(data=np.array(tcga), kernel=kernel, kernel_args=kernel_args)
    
    model = ICD(rank=rank)
    model.fit(K)
    G_icd = model.G
    #inxs = model.active_set_
    print("G shape:", G_icd.shape, "Error:", np.linalg.norm(K[:, :]-G_icd.dot(G_icd.T)))
    return model
    
# %%
    
def NystromOneKernelOneTCGA(tcga, kernel, kernel_args, rank):
    
    K = Kinterface(data=np.array(tcga), kernel=kernel, kernel_args=kernel_args)
    
    model = Nystrom(rank=rank)
    model.fit(K)
    G_nyst = model.G
    print "G shape:", G_nyst.shape, "Error:", np.linalg.norm(K[:, :]-G_nyst.dot(G_nyst.T))
    return model

# %%
def ridgeLowRankOneKernel(tcga, y_tr, kernel, kernel_args, rank, method):
    #K = Kinterface(data=X_tr, kernel=rbf_kernel, kernel_args={"sigma": 110})
    K = Kinterface(data=np.array(tcga), kernel=kernel, kernel_args=kernel_args)
    
    #for method in "nystrom", "icd":
    model = RidgeLowRank(method=method, rank=rank, lbd=1)
    model.fit([K], y_tr)
    #yp = model.predict([np.array(X_te)])
    #mse = mean_squared_error(y_te, yp)
    #rmse = np.var(y_te-yp)**0.5
    #print "Method:", method, "Test MSE:", mse
    return model

# %%
def variousKernelVariousMethodsOneTCGA(tcga, X_te, y_tr, y_te, method, rank):
    
    K_exp  = Kinterface(data=tcga, kernel=rbf_kernel,  kernel_args={"sigma": 30}) # RBF kernel 
    K_poly = Kinterface(data=tcga, kernel=poly_kernel, kernel_args={"degree": 3})      # polynomial kernel with degree=3
    K_lin  = Kinterface(data=tcga, kernel=linear_kernel)                          # linear kernel
    
    model = RidgeLowRank(method=method, rank=rank, lbd=1)
    model.fit([K_exp, K_lin, K_poly], y_tr)
    yp = model.predict([X_te, X_te, X_te])                     # The features passed to each kernel
    mse = mean_squared_error(y_te, yp)
    #rmse = np.var(y_tr-yp)**0.5
    print "Test MSE:", mse
    
# %%
def mklTCGA(mrna, cnv, meth, protein, dsurv, rank, sigmaKernel, degreeKernel, biasKernel, cKernel, sigmaABSKernel, sigmaPerKernel, nuKernel, L2Ridge, LookAhead):

    '''
    default parameters:
        rank = 15 ## 
        sigmaKernel = 2.0 ## exponential_kernel == rfb_kernel
        degreeKernel = 2 ## poly_kernel
        biasKernel = 0 ## linear_kernel
        cKernel = 1 ## sigmoid_kernel
        sigmaABSKernel = 2.0 ## exponential_absolute
        sigmaPerKernel = 1 ## periodic_kernel
        nuKernel = 1.5 ## matern_kernel
        L2Ridge = 0 ## lbd
        LookAhead = 10 ## delta
    '''
    def variousKernel(tcga, sigmaKernel, degreeKernel, biasKernel, cKernel, sigmaABSKernel, sigmaPerKernel, nuKernel):
        #Kernels
        K_exp  = Kinterface(data=np.array(tcga), kernel=rbf_kernel,  kernel_args={"sigma": sigmaKernel})  # RBF kernel 
        K_poly = Kinterface(data=np.array(tcga), kernel=poly_kernel, kernel_args={"degree": degreeKernel})  # polynomial kernel with degree=3
        K_lin  = Kinterface(data=np.array(tcga), kernel=linear_kernel, kernel_args={'b': biasKernel})
        K_sig  = Kinterface(data=np.array(tcga), kernel=sigmoid_kernel, kernel_args={'c': cKernel})
        K_expoAbs = Kinterface(data=np.array(tcga), kernel=exponential_absolute, kernel_args={"sigma": sigmaABSKernel})
        K_perio = Kinterface(data=np.array(tcga), kernel=periodic_kernel, kernel_args={"sigma": sigmaPerKernel})
        K_matern = Kinterface(data=np.array(tcga), kernel=matern_kernel, kernel_args={"nu": nuKernel})
        return K_exp, K_poly, K_lin, K_sig, K_expoAbs, K_perio ,K_matern
    
    K_exp_mrna, K_poly_mrna, K_lin_mrna, K_sig_mrna, K_expoAbs_mrna, K_perio_mrna, K_matern_mrna = variousKernel(mrna, sigmaKernel, degreeKernel, biasKernel, cKernel, sigmaABSKernel, sigmaPerKernel, nuKernel)
    K_exp_cnv, K_poly_cnv, K_lin_cnv, K_sig_cnv, K_expoAbs_cnv, K_perio_cnv, K_matern_cnv = variousKernel(cnv, sigmaKernel, degreeKernel, biasKernel, cKernel, sigmaABSKernel, sigmaPerKernel, nuKernel)
    K_exp_meth, K_poly_meth, K_lin_meth, K_sig_meth, K_expoAbs_meth, K_perio_meth, K_matern_meth = variousKernel(meth, sigmaKernel, degreeKernel, biasKernel, cKernel, sigmaABSKernel, sigmaPerKernel, nuKernel)
    K_exp_prot, K_poly_prot, K_lin_prot, K_sig_prot, K_expoAbs_prot, K_perio_prot, K_matern_prot = variousKernel(protein, sigmaKernel, degreeKernel, biasKernel, cKernel, sigmaABSKernel, sigmaPerKernel, nuKernel)
    
    model = Mklaren(rank=rank, lbd=L2Ridge, delta=LookAhead)
        
    model.fit([K_exp_mrna, K_poly_mrna, K_lin_mrna, K_sig_mrna, K_expoAbs_mrna, K_perio_mrna, K_matern_mrna,
               K_exp_cnv, K_poly_cnv, K_lin_cnv, K_sig_cnv, K_expoAbs_cnv, K_perio_cnv, K_matern_cnv, 
               K_exp_meth, K_poly_meth, K_lin_meth, K_sig_meth, K_expoAbs_meth, K_perio_meth, K_matern_meth, 
               K_exp_prot, K_poly_prot, K_lin_prot, K_sig_prot, K_expoAbs_prot, K_perio_prot, K_matern_prot],
    np.array(dsurv))
    
    #consolidado = []
    
#    #KFold para obtener los train y test de cada perfil
#    def kfoldProfiles(tcga, n_splits, seed): 
#        from sklearn.model_selection import KFold
#        kfold = KFold(n_splits=n_splits, random_state=seed, shuffle=True)
#        splits = kfold.split(tcga)
#        return splits
    
    #for ix_train, ix_test in kfoldProfiles(mrna, n_splits=5, seed=10):
        #from datasets.delve import load_boston
        #data = load_boston()
        
        #mrna_train=mrna[ix_train]
        #mrna_test=mrna[ix_test]
        #cnv_train=cnv[ix_train]
        #cnv_test=cnv[ix_test]
        #meth_train=meth[ix_train]
        #meth_test=meth[ix_test]
        #protein_train=protein[ix_train]
        #protein_test=protein[ix_test]
        #dsurv_train=np.array(dsurv)[ix_train]
        #dsurv_test=np.array(dsurv)[ix_test]
        
        
        
        #model = Mklaren(rank=rank, lbd=L2Ridge, delta=LookAhead)
        
        #model.fit([K_exp_mrna, K_poly_mrna, K_lin_mrna, K_sig_mrna, K_expoAbs_mrna, K_perio_mrna, K_matern_mrna,
                   #K_exp_cnv, K_poly_cnv, K_lin_cnv, K_sig_cnv, K_expoAbs_cnv, K_perio_cnv, K_matern_cnv, 
                   #K_exp_meth, K_poly_meth, K_lin_meth, K_sig_meth, K_expoAbs_meth, K_perio_meth, K_matern_meth, 
                   #K_exp_prot, K_poly_prot, K_lin_prot, K_sig_prot, K_expoAbs_prot, K_perio_prot, K_matern_prot],
        #np.array(dsurv_train))
        
        #yp = model.predict([mrna_test, mrna_test, mrna_test, mrna_test, mrna_test, mrna_test, mrna_test,
                            #cnv_test, cnv_test, cnv_test, cnv_test, cnv_test, cnv_test, cnv_test,
                            #meth_test, meth_test, meth_test, meth_test, meth_test, meth_test, meth_test, 
                            #protein_test, protein_test, protein_test, protein_test, protein_test, protein_test, protein_test])
        #print "Test MSE:", mean_squared_error(dsurv_test, yp)
        #consolidado.append(mean_squared_error(dsurv_test, yp))
    #print("Mean-> iternal train: " + str(np.mean(consolidado)) + " Variance: " + str(np.var(consolidado)))
    
    #yp = model.predict([mrna_te, mrna_te, mrna_te, mrna_te, cnv_te, cnv_te, cnv_te, cnv_te, meth_te, meth_te, meth_te, meth_te, protein_te, protein_te, protein_te, protein_te])
    
    #print "TestVal MSE:", mean_squared_error(dsurv_te, yp)
    
    return model

# %%
    
#    print(K.shape)
#    plt.figure()
#    plt.imshow(K[:, :])
#    plt.show()

## In[7]:
#
#
#print("The value of K at row 42 and column 66: ")
#print(K[42, 66])
#print 
#print("A 3x3 submatrix of K")
#print(K[:3, :3])
#
#print("Another 3x3 submatrix")
#print(K[[0, 10, 20], [1, 2, 5]])
#print 
#
#print("The value of K of sample 0 against samples 0, 1, 2 ..., 9: ")
#print(K[0, :10])
#print
#
#
## Similarly, the full kernel matrix can be generated.

# In[8]:

#
#plt.figure()
#plt.imshow(K[:, :])
#plt.show()


# <br/>
# ### Low-rank approximations to the kernel matrix

# The low rank approximations to ```K``` find a low rank matrix (termed $\mathbf{G}$) such that
# $$ \mathbf{K} = \mathbf{G}\mathbf{G}^T $$.

# <br/>
# <b>Incomplete Cholesky decomposition</b> finds $\mathbf{G}$ by greedily maximizing a lower-bound to the gain in approximation error (Fine, 2002).

# In[11]:

# <br/>
# <br/>
# <b>The Nystrom approximation</b> (Seeger, 2001) randomly selects a subset of active colums $\mathcal{A}$ and approximates
# $$ \mathbf{\hat{K}} = \mathbf{K}(:, \mathcal{A}) \mathbf{K}^{-1}(\mathcal{A}, \mathcal{A}) \mathbf{K}(:, \mathcal{A})^T   $$ 
# or in terms of $\mathbf{G}$:
# $$ \mathbf{G} = \mathbf{K}(:, \mathcal{A}) \mathbf{K}^{-1/2}(\mathcal{A}, \mathcal{A}) $$ 
# 

# In[12]:





# The approximations can be compared also visually.

# In[13]:


#fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(10, 6))
#ax[0].imshow(K[:, :])
#ax[0].set_title("Original")
#ax[1].imshow(G_icd.dot(G_icd.T))
#ax[1].set_title("ICD")
#ax[2].imshow(G_nyst.dot(G_nyst.T))
#ax[2].set_title("Nystroem")
#fig.tight_layout()
#plt.show()


# <br/>
# ### Low-rank Kernel Regression

# In[14]:


##from datasets.delve import load_boston
##data = load_boston()
#X = np.array(mrna)
#y = np.array(dsurv)
##print X.shape
#
#tr = range(0, X.shape[0], 2)
#te = range(1, X.shape[0], 2)
#X_tr, X_te = X[tr], X[te]
#y_tr, y_te = y[tr], y[te]


## In[15]:
#
#
#X.shape, y.shape
#
#
## In[16]:
#
#
#X_tr.shape, X_te.shape, y_tr.shape, y_te.shape


# In[17]:





# <br/>
# ### Multiple Kernel Learning

# #### Uniform kernel combination
# 
# Multiple kernels can be passed to ```RidgeLowRank``` model. This corresponds to an uniform combination of kernels in low-rank feature spaces.

# In[71]:


# 
# #### Linear combination of kernels
# 
# Alternatively, multiple kernel learning methods can be used to learn an optimal combination of kernels. The ```mklaren``` library implements methods based on centered kernel alignment (Cortes et. al, 2012). The following methods operate on <i>full kernel matrices</i>.
# 
# Given $p$ kernel matrices $\mathbf{K}_1, \mathbf{K}_2, ..., \mathbf{K}_p$, centered kernel alignment learns a linear combination of kernels resulting in a combined kernel matrix.
# 
# $$ \mathbf{K}_{c\mu} = \sum_{q=1}^p \mu_q \mathbf{K}_{cq} $$
# 
# where $\mathbf{K}_{cq}$ is the centered kernel matrix:
# $$\mathbf{K}_{cq} = (\mathbf{I} - \frac{\mathbf{11}^1}{n})\mathbf{K}_q (\mathbf{I} - \frac{\mathbf{11}^1}{n})$$
# 
# The following methods perform a constrained optimization over $\mathbf{\mu} = (\mu_1, \mu_2, ... \mu_p)$ maximizing the <i>centered alignment</i>:
# 
# $$ A = \frac{<\mathbf{K}_{c\mu}, \mathbf{y}\mathbf{y}^T>_F} {n <\mathbf{K}_{c\mu}, \mathbf{K}_{c\mu}>_F}  $$
# 
# where $\mathbf{y}\mathbf{y}^T$ is the <i>ideal kernel</i> based on target vector $\mathbf{y}$ and $<., .>_F$ is a matrix inner product (the <a href="https://en.wikipedia.org/wiki/Matrix_multiplication#Frobenius_product">Frobenius product</a>).

# 
# <b>align</b>. The ```align``` method sets each $\mu_q$ independently such that
# $$ \mu_q = <\mathbf{K}_{c\mu}, \mathbf{y}\mathbf{y}^T>_F^d $$
# with a user-defined parameter $d$.

# In[72]:


#from mklaren.mkl.align import Align
#model = Align()
#model.fit([K_exp, K_lin, K_poly], y_tr)
#model.mu  # kernel weights


# <b>alignf</b>. The ```alignf``` method optimizes with respect to $\mathbf{\mu}$ to maximize centered alignment.
# $$ \text{max}_{\mathbf{\mu}} \frac{<\mathbf{K}_{c\mu}, \mathbf{y}\mathbf{y}^T>_F} {n <\mathbf{K}_{c\mu}, \mathbf{K}_{c\mu}>_F}   $$
# 
# such that (```typ=linear```):
# $$ \sum \mu_q = 1$$
# 
# or contraining sum of wieghts to a convex combination (```typ=convex```):
# $$ \sum \mu_q = 1 \text{ and } \mu_q > 0, \forall q$$

# In[73]:
#
#
#from mklaren.mkl.alignf import Alignf
#model = Alignf(typ="linear")
#model.fit([K_exp, K_lin, K_poly], y_tr)
#model.mu  # kernel weights


# In[74]:

#
#model = Alignf(typ="convex")
#model.fit([K_exp, K_lin, K_poly], y_tr)
#model.mu  # kernel weights (convex combination)
#

# #### Regression with MKL
# 
# The ```Kinterface``` class provide a callable. A new kernel function is a linear combination of base kernels. 

# In[75]:

#
#mu = model.mu
#combined_kernel = lambda x, y:     mu[0] * K_exp(x, y) + mu[1] * K_lin(x, y) + mu[2] * K_poly(x, y)
#    
#combined_kernel(np.array([0, 1, 2]), np.array([2, 1, 0]), )


# The obtained callable can be used e.g. in ```sklearn``` kernel methods.

# In[76]:

#
#from sklearn.kernel_ridge import KernelRidge
#krr = KernelRidge(kernel=combined_kernel, alpha=10.0)
#krr.fit(X_tr, y_tr)
#yp = krr.predict(X_te)
#print "Test RMSE:", np.var(y_te-yp)**0.5


# <br/>
# ### Simultaneous Multiple Kernel regression and low-rank approximation
# 
# The Mklaren algorithm (Multiple kernel learning with least-angle regression) perform simultaneous low-rank approximation of multiple kernel learning using least-angle regression (Stražar & Curk, 2016).

# In[63]:





# Kernel more related to targets are approximated with greater accuracy.

# In[77]:


#G_exp = model.data[0]["G"]
#G_lin = model.data[1]["G"]
#G_poly = model.data[2]["G"]
#
#fig, ax = plt.subplots(nrows=2, ncols=3, figsize=(8, 4))
#for i, (name, K, G) in enumerate(zip(["exp", "lin", "poly"], 
#                               [K_exp, K_lin, K_poly], 
#                               [G_exp, G_lin, G_poly])):
#    ax[0, i].set_title(name)
#    ax[0, i].imshow(K[:, :])
#    ax[1, i].imshow(G.dot(G.T))
#
#ax[0, 0].set_ylabel("Original")
#ax[1, 0].set_ylabel("Mklaren")
#fig.tight_layout()
#plt.show()


# ### References
# 
# 1. M. Stražar and T. Curk, “Learning the kernel matrix via predictive low-rank approximations,” arXiv Prepr. arXiv1601.04366, 2016.
# 2. C. Cortes, M. Mohri, and A. Rostamizadeh, “Algorithms for Learning Kernels Based on Centered Alignment,” J. Mach. Learn. Res., vol. 13, pp. 795–828, Mar. 2012.
# 3.  F. R. Bach and M. I. Jordan, “Predictive low-rank decomposition for kernel methods,” Proceedings of the 22nd international conference on Machine learning - ICML ’05. ACM Press, New York, New York, USA, pp. 33–40, 2005.
# 4. S. Fine and K. Scheinberg, “Efficient SVM Training Using Low-Rank Kernel Representations,” J. Mach. Learn. Res., vol. 2, pp. 243–264, 2001.
# 5. C. Williams and M. Seeger, “Using the Nystr{ö}m method to speed up kernel machines,” in Proceedings of the 14th Annual Conference on Neural Information Processing Systems, 2001, no. EPFL-CONF-161322, pp. 682–688.

